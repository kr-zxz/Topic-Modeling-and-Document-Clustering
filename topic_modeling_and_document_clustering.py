# -*- coding: utf-8 -*-
"""Topic Modeling and  Document Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PS1-g4Y9Z_cla36ZaZ6YRbL8JoYgeyhR
"""



!pip install pyLDAvis

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.datasets import fetch_20newsgroups
import gensim
from gensim import corpora
from gensim.models import LdaModel
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from gensim.models import Word2Vec
import numpy as np

# Download NLTK resources if not already downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Load the 20 Newsgroups dataset
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data

# Text Preprocessing
def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase
    tokens = [token for token in tokens if token.isalpha()]  # Remove punctuation
    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords
    return tokens

preprocessed_documents = [preprocess_text(doc) for doc in documents]

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(preprocessed_documents)
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]

# Train the LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=42)

# Print the topics found by the LDA model
print("LDA Topics:")
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic: {idx} \nWords: {topic}")

# Prepare the visualization
lda_display = gensimvis.prepare(lda_model, corpus, dictionary)

import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Prepare the LDA visualization data
lda_display = gensimvis.prepare(lda_model, corpus, dictionary)

# Display the visualization inline in the notebook
pyLDAvis.display(lda_display)

# Train Word2Vec model on the preprocessed documents
word2vec_model = Word2Vec(sentences=preprocessed_documents, vector_size=100, window=5, min_count=5, workers=4)

# Represent each document as the average of word vectors
def document_vector(doc):
    # Remove out-of-vocabulary words
    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]
    if len(doc) > 0:
        return np.mean(word2vec_model.wv[doc], axis=0)
    else:
        return np.zeros(word2vec_model.vector_size)

# Get vectors for each document
doc_vectors = [document_vector(doc) for doc in preprocessed_documents]

# Calculate cosine similarity between document vectors
cosine_similarities = cosine_similarity(doc_vectors)

# Cluster documents using KMeans
num_clusters = 10
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(doc_vectors)
labels = kmeans.labels_

# Get vectors for each document
doc_vectors = [document_vector(doc) for doc in preprocessed_documents]

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming 'doc_vectors' contains the document vectors and 'labels' contains the cluster labels
pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(doc_vectors)

plt.figure(figsize=(10, 8))
plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=labels, cmap='rainbow', edgecolor='k', s=50)
plt.title('Document Clustering using PCA and KMeans')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster Label')
plt.grid(True)
plt.show()

# Comparison of document similarity using LDA-based vectors
lda_doc_vectors = [lda_model.get_document_topics(bow, minimum_probability=0.0) for bow in corpus]
lda_doc_vectors = [[prob for topic, prob in doc] for doc in lda_doc_vectors]

# Remove any empty vectors
lda_doc_vectors = np.array([vec for vec in lda_doc_vectors if vec])

# Calculate cosine similarity for LDA-based vectors
lda_cosine_similarities = cosine_similarity(lda_doc_vectors)

print("\nCosine Similarity using LDA-based Vectors:")
print(lda_cosine_similarities)

